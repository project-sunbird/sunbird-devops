- hosts: local
  gather_facts: no
  vars_files:
    - "{{inventory_dir}}/secrets.yml"
  # The vars: section is added for the below reason
  # 1. Introduce a common variable for various clouds. In case of azure, it refers to container name, in case of aws / gcp, it refers to folder name
  # 2. We want to avoid too many new variable introduction / replacement in first phase. Hence we will reuse the existing variable defined in private repo
  #    or other default files and just assign the value to the newly introduced common variable 
  # 3. After few releases, we will remove the older variables and use only the new variables across the repos
  vars:
    dial_plugin_storage: "{{ dial_plugin_container_name }}"
  tasks:
    - name: Create directories
      file:
        path: dial_schema_template_files/{{ item.path }}
        state: directory
        owner: jenkins
        group: jenkins
      with_filetree: "{{ source_name }}"
      when: item.state == 'directory'

    - name: Template files in all directories
      template:
        src: "{{ item.src }}"
        dest: dial_schema_template_files/{{ item.path }}
      with_filetree: "{{ source_name }}"
      when: item.state == 'file'

    - name: upload batch of files to azure storage
      include_role:
        name: azure-cloud-storage
        tasks_from: blob-upload-batch.yml
      vars:
        blob_container_name: "{{ dial_plugin_storage }}"
        container_public_access: "blob"
        blob_container_folder_path: "/schemas/local"
        local_file_or_folder_path: "dial_schema_template_files"
        storage_account_name: "{{ azure_public_storage_account_name }}"
        storage_account_key: "{{ azure_public_storage_account_key }}"
      when: cloud_service_provider == "azure"
    
    - name: upload batch of files to aws s3
      include_role:
        name: aws-cloud-storage
        tasks_from: upload-folder.yml
      vars:
        s3_bucket_name: "{{ aws_public_s3_bucket_name }}"
        aws_access_key_id: "{{ aws_public_bucket_access_key }}"
        aws_secret_access_key: "{{ aws_public_bucket_secret_access_key }}"
        aws_default_region: "{{ aws_region }}"
        local_file_or_folder_path: "dial_schema_template_files"
        s3_path: "{{ dial_plugin_storage }}/schemas/local"
      when: cloud_service_provider == "aws"

    - name: upload batch of files to gcloud storage
      include_role:
              name: gcp-cloud-storage
              tasks_from: upload-batch.yml
      vars:
        dest_folder_name: "{{ dial_plugin_storage }}"
        dest_folder_path: "schemas/local"
        local_file_or_folder_path: "dial_schema_template_files"
        gcp_bucket_name: "{{ gcloud_public_bucket_name }}"
      when: cloud_service_provider == "gcloud"

