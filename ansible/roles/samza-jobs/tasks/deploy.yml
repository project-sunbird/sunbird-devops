---
- name: Create Directory for Jobs
  file: path={{item}} owner=hduser group=hadoop recurse=yes state=directory
  with_items:
    - "{{samza_jobs_dir}}"
    - "{{samza_jobs_dir}}/extract"

- name: Copy script to get all running jobs
  copy: src=get_all_running_app_name.sh dest=/usr/local/hadoop/bin owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to get all job names
  copy: src=get_all_job_name.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to get updated job names from extracted tar
  copy: src=update_new_job_name.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to start jobs based on the status
  copy: src=start_jobs.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to remove old job tar
  copy: src=remove_old_tar.sh dest="{{samza_jobs_dir}}/extract" owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Copy script to kill jobs based on the status
  copy: src=kill_jobs.sh dest=/usr/local/hadoop/bin owner=hduser group=hadoop mode="u=rwx,g=rx,o=r"

- name: Remove file of job status
  file: path="{{job_status_file}}" state=absent

- name: Get job names from folder
  command: bash -lc "./get_all_job_name.sh {{job_status_file}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract"

- name: Ensure yarn resource manager is running
  command: bash -lc "(ps aux | grep yarn-hduser-resourcemanager | grep -v grep) || /usr/local/hadoop/sbin/yarn-daemon.sh --config /usr/local/hadoop-{{hadoop_version}}/conf/ start resourcemanager"
  become: yes
  become_user: hduser

- name: Update status of running job in file
  command: bash -lc "./get_all_running_app_name.sh {{job_status_file}}"
  args:
    chdir: /usr/local/hadoop/bin

- name: copy new jobs tar ball
  copy: src={{ item }} dest={{samza_jobs_dir}}/ force=no owner=hduser group=hadoop
  with_fileglob:
    - ./jobs/*
  register: new_jobs

- name: Create Directory to extract new jobs
  file: path={{samza_jobs_dir}}/extract/{{item.item | basename }} owner=hduser group=hadoop recurse=yes state=directory
  register: extract_dir
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: extract new jobs
  command: tar -xvf "{{samza_jobs_dir}}/{{item.item | basename}}" -C "{{samza_jobs_dir}}/extract/{{item.item | basename }}"
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: Create Directory to extract new jobs
  file: path={{samza_jobs_dir}}/extract/ owner=hduser group=hadoop recurse=yes

- name: Get all new job configs
  shell: "ls -d -1 {{item.path}}/config/*.properties"
  register: config_files
  when: "{{item|changed}}"
  with_items: "{{ (extract_dir|default({})).results|default([]) }}"


- name: update environment specific details in new job configs
  replace: dest="{{item[1].stdout}}" regexp="{{item[0].key}}" replace="{{item[0].value}}"
  when: "{{item[1]|changed}}"
  with_nested:
    - [{key: "__yarn_host__", value: "{{__yarn_host__}}"}, {key: "__yarn_port__", value: "{{__yarn_port__}}"}, {key: "__env__", value: "{{env_name}}" }, {key: "__zookeepers__", value: "{{zookeepers}}"}, {key: "__kafka_brokers__", value: "{{kafka_brokers}}"}, {key: "__lms_host__", value: "{{__lms_host__}}"}, {key: "__lms_es_port__", value: "{{sunbird_es_port}}"}, {key: "__lms_es_host__", value: "{{sunbird_es_host}}"}]
    - "{{ (config_files|default({})).results|default([]) }}"

- name: Update status of new jobs in file
  command: bash -lc "./update_new_job_name.sh {{job_status_file}} {{samza_jobs_dir}}/extract/{{item.item | basename}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"
  when: "{{item|changed}}"
  with_items: "{{ (new_jobs|default({})).results|default([]) }}"

- name: Kill jobs
  command: bash -lc "./kill_jobs.sh {{job_status_file}}"
  args:
    chdir: /usr/local/hadoop/bin

- name: Start jobs
  command: bash -lc "./start_jobs.sh {{job_status_file}} {{samza_jobs_dir}}/extract"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"
  become_user: hduser

- name: Remove all old tar
  command: bash -lc "./remove_old_tar.sh {{job_status_file}} {{samza_jobs_dir}}"
  args:
    chdir: "{{samza_jobs_dir}}/extract/"

- file: path={{samza_jobs_dir}} owner=hduser group=hadoop state=directory recurse=yes
