- name: ensure backup dir exists
  file: path="{{ postgresql_backup_dir }}" state=directory owner={{ postgresql_user }} group={{ postgresql_user }}

- set_fact:
    postgresql_backup_gzip_file_name: "postgresql_backup_{{ lookup('pipe', 'date +%Z-%Y-%m-%d-%H-%M-%S') }}.sql.gz"

- set_fact:
    postgresql_backup_gzip_file_path: "{{ postgresql_backup_dir }}/{{ postgresql_backup_gzip_file_name }}"

- name: Save backup
  command: bash -lc "pg_dumpall | gzip > {{ postgresql_backup_gzip_file_path }}"
  become_user: "{{ postgresql_user }}"
  async: 3600
  poll: 10

- name: upload file to azure storage
  include_role:
    name: azure-cloud-storage
    tasks_from: blob-upload.yml
  vars:
    blob_container_name: "{{ postgresql_backup_storage }}"
    container_public_access: "off"
    blob_file_name: "{{ postgresql_backup_gzip_file_name }}"
    local_file_or_folder_path: "{{ postgresql_backup_gzip_file_path }}"
    storage_account_name: "{{ azure_management_storage_account_name }}"
    storage_account_key: "{{ azure_management_storage_account_key }}"
  when: cloud_service_provider == "azure"

- name: upload file to aws s3
  include_role:
    name: aws-cloud-storage
    tasks_from: upload.yml
  vars:
    s3_bucket_name: "{{ aws_management_s3_bucket_name }}"
    aws_access_key_id: "{{ aws_management_bucket_access_key }}"
    aws_secret_access_key: "{{ aws_management_bucket_secret_access_key }}"
    aws_default_region: "{{ aws_region }}"
    local_file_or_folder_path: "{{ postgresql_backup_gzip_file_path }}"
    s3_path: "{{ postgresql_backup_storage }}/{{ postgresql_backup_gzip_file_name }}"
  when: cloud_service_provider == "aws"   

- name: clean up backup dir after upload
  file: path="{{ postgresql_backup_dir }}" state=absent