application.env="{{ env_name }}"
spark.cassandra.connection.host={{groups['dp-cassandra'][0]}}
spark.cassandra.connection.port="9042"
# Data Exhaust API
data_exhaust.list.limit="100"
data_exhaust.retry.limit="3"
data_exhaust.dataset.list=["eks-consumption-raw", "eks-consumption-summary", "eks-consumption-metrics","eks-creation-raw", "eks-creation-summary", "eks-creation-metrics"]
data_exhaust.dataset.default="eks-consumption-raw"
data_exhaust.output_format="json"
data_exhaust.bucket="reports"
cassandra.service.embedded.enable=false
cassandra.keyspace_prefix="{{ cassandra.keyspace_prefix }}"
device-register-controller-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 4
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 8
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}
device-register-actor-dispatcher {
  type = "Dispatcher"
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}
device-profile-actor-dispatcher {
  type = "Dispatcher"
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}
experiment-actor {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}
default-dispatcher {
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}
#AKKA Configuration
akka {
  actor {
    deployment {
        /druid-health-actor  {
            router = smallest-mailbox-pool
            nr-of-instances = 10
        }
        /job-service-actor {
          router = smallest-mailbox-pool
            nr-of-instances = 10
        }
        /device-register-actor {
          router = smallest-mailbox-pool
          dispatcher = device-register-actor-dispatcher
          nr-of-instances = 6
        }
        /device-profile-actor {
          router = smallest-mailbox-pool
          dispatcher = device-profile-actor-dispatcher
          nr-of-instances = 6
        }
        /client-log-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 4
        }
        /experiment-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 2
        }
    }
  }
}
#Netty Configuration
play.server {
  # The server provider class name
  provider = "play.core.server.NettyServerProvider"
  netty {
    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 30
    # Whether the Netty wire should be logged
    log.wire = true
    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux
    transport = "native"
  }
}
play.modules.enabled+="modules.ActorInjector"
play.filters.enabled+="filter.RequestInterceptor"
play.filters.enabled+="play.filters.hosts.AllowedHostsFilter"
play.filters.disabled+="play.filters.csrf.CSRFFilter"
# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
play.http.secret.key="{{dp_play_http_secret_key}}"
play.filters.hosts {
   # A list of valid hosts (e.g. "example.com") or suffixes of valid hosts (e.g. ".example.com")
   # Note that ".example.com" will match example.com and any subdomain of example.com, with or without a trailing dot.
   # "." matches all domains, and "" matches an empty or nonexistent host.
  allowed = ["."]
}
# body parser
play.http.parser.maxMemoryBuffer=10M
default.consumption.app.id="no_value"
default.channel.id="{{default_channel_id}}"
default.creation.app.id="no_value"
postgres.db="{{postgres.db_name}}"
postgres.url="jdbc:postgresql://{{ postgres.db_url }}:5432/"
postgres.user="{{ postgres.db_username }}"
postgres.pass="{{ postgres.db_password }}"
postgres.table_name="{{ postgres.db_table_name }}"
postgres.table.geo_location_city.name="{{ geo_location_city }}"
postgres.table.geo_location_city_ipv4.name="{{ geo_location_city_ipv4 }}"
postgres.table.report_config.name="{{ report_config }}"
postgres.table.job_request.name="{{ job_request }}"
postgres.table.experiment_definition.name="{{ experiment_definition }}"
default.channel="{{ default_channel }}"
elasticsearch.service.endpoint="http://{{groups['composite-search-cluster'][0]}}:9200"
elasticsearch.index.compositesearch.name="compositesearch"
elasticsearch.index.dialcodemetrics.name="dialcodemetrics"
metrics.dialcodemetrics.request.limit=1000
dataexhaust.authorization_check=true
user.profile.url="{{ user_profile_read_url }}"
org.search.url="{{ org_search_url }}"
standard.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN"]
ondemand.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN","CONTENT_CREATOR","COURSE_MENTOR"]
dataexhaust.super.admin.channel="{{ dataexhaust_super_admin_channel }}"
storage-service.request-signature-version="AWS4-HMAC-SHA256"
s3service.region="ap-south-1"
#channel exhaust configs
channel {
  data_exhaust {
    whitelisted.consumers={{ exhaust_api_consumer_ids }}
    expiryMins = 30
    dataset {
      default {
        bucket = "{{channel_data_exhaust_bucket}}"
        basePrefix = "data-exhaust/"
      }
      raw {
        bucket = "{{channel_data_exhaust_bucket}}"
        basePrefix = "data-exhaust/"
      }
      summary {
        bucket = "{{channel_data_exhaust_bucket}}"
        basePrefix = "data-exhaust/"
      }
      summary-rollup {
        bucket = "{{channel_data_exhaust_bucket}}"
        basePrefix = "data-exhaust/"
      }
    }
  }
}
cloud_storage_type="azure"
storage.key.config="azure_storage_key"
storage.secret.config="azure_storage_secret"
metrics.time.interval.min=30
cache.refresh.time.interval.min=1440
redis.host="{{metadata_redis_host}}"
#redis.host="localhost"
redis.port={{ device_db_redis_port }}
#redis.port=__redis_port__
redis.connection.max=20
redis.connection.idle.max=20
redis.connection.idle.min=10
redis.connection.minEvictableIdleTimeSeconds=120
redis.connection.timeBetweenEvictionRunsSeconds=300
elasticsearch.host="{{lp_composite_search_host}}"
elasticsearch.port=9200
# experiment service settings
elasticsearch.searchExperiment.index="experiment"
elasticsearch.searchExperiment.fieldWeight="{\"userId\":3.0, \"deviceId\":3.0, \"url\":3.0 }"
elasticsearch.searchExperiment.matchQueryScore=9.0
deviceRegisterAPI.experiment.enable=false
experimentService.redisEmptyValueExpirySeconds=86400
redis.experimentIndex=9
redis.deviceIndex=2
druid.coordinator.host="http://{{groups['raw-coordinator'][0]}}:8081/"
druid.healthcheck.url="druid/coordinator/v1/loadstatus"
device.api.enable.debug.log=true
kafka.device.register.topic={{ env_name }}.events.deviceprofile
kafka.metrics.event.topic={{ env_name }}.pipeline_metrics
kafka.broker.list="{{ ingestion_kafka_brokers }}"
