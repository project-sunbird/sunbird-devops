# This is the main configuration file for the application.
# https://www.playframework.com/documentation/latest/ConfigFile
# ~~~~~
# Play uses HOCON as its configuration file format.  HOCON has a number
# of advantages over other config formats, but there are two things that
# can be used when modifying settings.
#
# You can include other configuration files in this main application.conf file:
#include "extra-config.conf"
#
# You can declare variables and substitute for them:
#mykey = ${some.value}
#
# And if an environment variable exists when there is no other substitution, then
# HOCON will fall back to substituting environment variable:
#mykey = ${JAVA_HOME}

## Akka
# https://www.playframework.com/documentation/latest/ScalaAkka#Configuration
# https://www.playframework.com/documentation/latest/JavaAkka#Configuration
# ~~~~~
# Play uses Akka internally and exposes Akka Streams and actors in Websockets and
# other streaming HTTP responses.

akka {
  default-dispatcher {
    executor = "fork-join-executor"
    fork-join-executor {
      # The parallelism factor is used to determine thread pool size using the
      # following formula: ceil(available processors * factor). Resulting size
      # is then bounded by the parallelism-min and parallelism-max values.
      parallelism-factor = 3.0

      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 8

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 16
    }
    # Throughput for default Dispatcher, set to 1 for as fair as possible
    throughput = 1
  }

  view-collect-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      # The parallelism factor is used to determine thread pool size using the
      # following formula: ceil(available processors * factor). Resulting size
      # is then bounded by the parallelism-min and parallelism-max values.
      parallelism-factor = 3.0

      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 16
    }
    # Throughput for default Dispatcher, set to 1 for as fair as possible
    throughput = 1
  }

  view-provide-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      # The parallelism factor is used to determine thread pool size using the
      # following formula: ceil(available processors * factor). Resulting size
      # is then bounded by the parallelism-min and parallelism-max values.
      parallelism-factor = 2.0

      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2

      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 4
    }
    # Throughput for default Dispatcher, set to 1 for as fair as possible
    throughput = 1
  }
  actor {
    deployment {
        /health-check-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 1
        }
        /view-collect-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 1
          dispatcher= view-collect-dispatcher
        }
        /view-read-actor {
          router = smallest-mailbox-pool
          nr-of-instances = 1
          dispatcher = view-provide-dispatcher
        }
    }
  }
}


#Netty Configuration
play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 30

    # Whether the Netty wire should be logged
    log.wire = true

    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux
    transport = "jdk"
  }
}
akka.loglevel = DEBUG

play.modules.enabled+="modules.ActorInjector"
play.filters.enabled+="filter.RequestInterceptor"
env=local

log4j.appender.kafka.enable="true"
log4j.appender.kafka.broker_host={{kafka_urls}}
log4j.appender.kafka.topic="viewer.telemetry.log"

{% if (cassandra_cluster_size | int) > 1 %}
cassandra.connection.host={{groups['cassandra']|join(',')}}
cassandra.connection.port=9042,9042,9042
{% else %}
cassandra.connection.host={{sunbird_cassandra_host}}
cassandra.connection.port=9042
{% endif %}

kafka.broker.list={{kafka_urls}}
kafka_topics_instruction={{kafka_topics_instruction}}
accesstoken.publickey.basepath={{lms_device_basepath | default('/keys/')}}
redis {
  host = {{ sunbird_redis_host }}
  port = {{ sunbird_redis_port | default(6379) }}
  connection {
    max = {{ viewer_redis_connection_max | default(64) }}
    idle.max = {{  viewer_redis_connection_idle_max | default(32) }}
    idle.min = {{  viewer_redis_connection_idle_min | default(1) }}
    minEvictableIdleTimeSeconds= {{  viewer_redis_connection_minEvictableIdleTimeSeconds | default(120) }}
    timeBetweenEvictionRunsSeconds = {{  viewer_redis_connection_timeBetweenEvictionRunsSeconds | default(300) }}
   }
}

service {
  search {
    base_path = {{ sunbird_search_service_api_base_url | default('http://search-service:9000') }}
    api_url = "/v3/search"
  }

}