- name: ensure backup base directory exists
  file: path={{ jenkins_backup_base_dir }} state=directory owner={{ jenkins_user }} group={{ jenkins_group }}

- name: Find latest directory
  shell: ls -dt */ | head -n 1 | cut -d'/' -f1
  args:
    chdir: "{{ jenkins_backup_base_dir }}"
  register: LATEST_BACKUP_DIR

- debug: msg={{ LATEST_BACKUP_DIR.stdout }}

- name: Create archive of backup directory
  archive: path="{{ jenkins_backup_base_dir }}/{{ LATEST_BACKUP_DIR.stdout }}" dest="/tmp/{{ LATEST_BACKUP_DIR.stdout }}.zip" format=zip

- name: upload file to azure storage
  include_role:
    name: azure-cloud-storage
    tasks_from: blob-upload.yml
  vars:
    blob_container_name: "{{ jenkins_backup_storage }}"
    container_public_access: "off"
    blob_file_name: "{{ LATEST_BACKUP_DIR.stdout }}.zip"
    local_file_or_folder_path: "/tmp/{{ LATEST_BACKUP_DIR.stdout }}.zip"
    storage_account_name: "{{ azure_management_storage_account_name }}"
    storage_account_key: "{{ azure_management_storage_account_key }}"
  when: cloud_service_provider == "azure"

- name: upload file to aws s3
  include_role:
    name: aws-cloud-storage
    tasks_from: upload.yml
  vars:
    s3_bucket_name: "{{ aws_management_s3_bucket_name }}"
    aws_access_key_id: "{{ aws_management_bucket_access_key }}"
    aws_secret_access_key: "{{ aws_management_bucket_secret_access_key }}"
    aws_default_region: "{{ aws_region }}"
    local_file_or_folder_path: "/tmp/{{ LATEST_BACKUP_DIR.stdout }}.zip"
    s3_path: "{{ jenkins_backup_storage }}/{{ LATEST_BACKUP_DIR.stdout }}.zip"
  when: cloud_service_provider == "aws"
  
- name: upload file to gcloud storage
  include_role:
    name: gcp-cloud-storage
    tasks_from: upload.yml
  vars:
    gcp_bucket_name: "{{ gcloud_management_bucket_name }}"
    dest_folder_name: "{{ jenkins_backup_storage  }}"
    dest_file_name: "{{ LATEST_BACKUP_DIR.stdout }}.zip"
    local_file_or_folder_path: "/tmp/{{ LATEST_BACKUP_DIR.stdout }}.zip"
  when: cloud_service_provider == "gcloud"

