version: "3.3"


services:
    prometheus:
        image: quay.io/prometheus/prometheus:v1.7.1
        ports:
            - "9090:9090"
        networks:
            - monitoring
        volumes:
            - /var/dockerdata/prometheus/data:/prometheus
        command: "-config.file=/etc/prometheus/prometheus.yml -alertmanager.url http://alertmanager:9093/alertmanager -storage.local.path=/prometheus -web.console.libraries=/etc/prometheus/console_libraries -storage.local.target-heap-size=157286400 -storage.local.retention={{prometheus_storage_retention_time}} -web.console.templates=/etc/prometheus/consoles -web.route-prefix={{prometheus_route_prefix}} -web.external-url={{prometheus_web_external_url}}"
        configs:
            - source: prometheus.yml
              target: /etc/prometheus/prometheus.yml
            - source: alertrules.nodes
              target: /etc/prometheus-rules/alertrules.nodes
            - source: alertrules.task
              target: /etc/prometheus-rules/alertrules.task
            - source: alertrules.es
              target: /etc/prometheus-rules/alertrules.es
            - source: alertrules.logs
              target: /etc/prometheus-rules/alertrules.logs
            - source: alertrules.backups
              target: /etc/prometheus-rules/alertrules.backups
            - source: alertrules.postgresql
              target: /etc/prometheus-rules/alertrules.postgresql
            - source: alertrules.services
              target: /etc/prometheus-rules/alertrules.services
            - source: alertrules.kong
              target: /etc/prometheus-rules/alertrules.kong
            - source: alertrules.docker
              target: /etc/prometheus-rules/alertrules.docker
        deploy:
            replicas: 1
            placement:
                constraints:
                    - "node.labels.prometheus==1"
            resources:
              reservations:
                memory: "{{ prometheus.reservation_memory | default('2G') }}"
              limits:
                memory: "{{ prometheus.limit_memory | default('4G') }}"

    alertmanager:
        image: prom/alertmanager:v0.8.0
        ports:
            - "9093:9093"
        networks:
            - monitoring
        volumes:
            - /var/dockerdata/alertmanager/data:/etc/alertmanager/data
        command: "-config.file=/etc/alertmanager/alertmanagerconfig.yml -storage.path=/alertmanager/data -web.route-prefix={{prometheus_alertmanager_route_prefix}} -web.external-url={{prometheus_alertmanager_web_external_url}}"
        configs:
            - source: alertmanagerconfig.yml
              target: /etc/alertmanager/alertmanagerconfig.yml
        deploy:
            replicas: 1
            placement:
                constraints:
                    - "node.labels.alertmanager==1"

    blackbox-exporter:
        image: quay.io/prometheus/blackbox-exporter:v0.8.1
        ports:
            - "9115:9115"
        command: "--config.file=/etc/blackbox-exporter/blackboxconfig.yml"
        networks:
            - monitoring
            - api-manager_default
            - application_default
            - logger
        configs:
            - source: blackboxconfig.yml
              target: /etc/blackbox-exporter/blackboxconfig.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ blackbox_exporter.reservation_memory }}"
                limits:
                    memory: "{{ blackbox_exporter.limit_memory }}"

    cadvisor:
        image: blep/cadvisor_bugfix1556:v0.26.1
        ports:
            - "8081:8080"
        networks:
            - monitoring
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
        deploy:
            mode: global
            resources:
                reservations:
                    memory: "{{ cadvisor.reservation_memory }}"
                limits:
                    memory: "{{ cadvisor.limit_memory }}"

    node-exporter:
        image: basi/node-exporter:v0.1.1
        ports:
            - "9100:9100"
        networks:
            - monitoring
        environment:
            - HOST_HOSTNAME=/etc/hostname
        volumes:
            - /proc:/host/proc
            - /sys:/host/sys
            - /:/rootfs
            - /etc/hostname:/etc/hostname
        command: [ -collector.procfs=/host/proc,-collector.sysfs=/host/proc,-collector.filesystem.ignored-mount-points="^/(sys|proc|dev|host|etc)($$|/)",-collector.textfile.directory=/etc/node-exporter/]
        deploy:
            mode: global
            resources:
                reservations:
                    memory: "{{ node_exporter.reservation_memory }}"
                limits:
                    memory: "{{ node_exporter.limit_memory }}"

    elasticsearch_exporter:
        image: justwatch/elasticsearch_exporter:1.0.1
        ports:
            - "9108:9108"
        networks:
            - monitoring
        command:
            - '-es.uri=http://{{ monitor_es_host }}:9200'
            - '-es.all=true'
        deploy:
            replicas: 1
            resources:
                reservations:
                    memory: "{{ elasticsearch_exporter.reservation_memory }}"
                limits:
                    memory: "{{ elasticsearch_exporter.limit_memory }}"

{% if groups['log-es'] %}
    log_elasticsearch_exporter:
        image: justwatch/elasticsearch_exporter:1.0.1
        ports:
            - "9109:9108"
        networks:
            - monitoring
        command:
            - '-es.uri=http://{{ groups['log-es'][0] }}:9200'
            - '-es.all=true'
        deploy:
            replicas: 1
            resources:
                reservations:
                    memory: "{{ elasticsearch_exporter.reservation_memory }}"
                limits:
                    memory: "{{ elasticsearch_exporter.limit_memory }}"
{% endif %}

    master_postgres_exporter:
        image: wrouesnel/postgres_exporter:v0.2.2
        ports:
            - 9187:9187
        networks:
            - monitoring
        command:
            - '-extend.query-path=/etc/postgres_exporter/postgresmasterqueries.yml'
        environment:
            - DATA_SOURCE_NAME=postgresql://{{ postgres_exporter_user }}:{{ postgres_exporter_password }}@{{ groups['postgresql-master'][0]}}:{{ postgres_exporter_postgres_port }}/postgres?sslmode=disable
        configs:
            - source: postgresmasterqueries.yml
              target: /etc/postgres_exporter/postgresmasterqueries.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ postgres_exporter.reservation_memory }}"
                limits:
                    memory: "{{ postgres_exporter.limit_memory }}"


    {% if groups['postgresql-slave'][0] is defined %}
    # This empty line ensures indentation is correct after ansible jinja2 template is materialized
    slave_postgres_exporter:
        image: wrouesnel/postgres_exporter:v0.2.2
        ports:
            - 9188:9187
        networks:
            - monitoring
        command:
            - '-extend.query-path=/etc/postgres_exporter/postgresslavequeries.yml'
        environment:
            - DATA_SOURCE_NAME=postgresql://{{ postgres_exporter_user }}:{{ postgres_exporter_password }}@{{ groups['postgresql-slave'][0]}}:{{ postgres_exporter_postgres_port }}/postgres?sslmode=disable
        configs:
            - source: postgresslavequeries.yml
              target: /etc/postgres_exporter/postgresslavequeries.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ postgres_exporter.reservation_memory }}"
                limits:
                    memory: "{{ postgres_exporter.limit_memory }}"
    {% endif %}

    statsd_exporter:
        image: prom/statsd-exporter:master
        ports:
            - 9102:9102
            - 9125:9125
            - 9125:9125/udp
        networks:
            - monitoring
            - api-manager_default
        command:
            - '-statsd.mapping-config=/etc/statsd_exporter/statsd_mapping.yml'
        configs:
            - source: statsd_mapping.yml
              target: /etc/statsd_exporter/statsd_mapping.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ statsd_exporter.reservation_memory }}"
                limits:
                    memory: "{{ statsd_exporter.limit_memory }}"

    kong_cluster_exporter:
        image: sunbird/prometheus-jsonpath-exporter:v0.0.1
        ports:
            - 9158:9158
        networks:
            - monitoring
            - api-manager_default
        command: /etc/kong_cluster_exporter/config.yml
        configs:
            - source: kong_cluster_exporter_config.yml
              target: /etc/kong_cluster_exporter/config.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ jsonpath_exporter.reservation_memory }}"
                limits:
                    memory: "{{ jsonpath_exporter.limit_memory }}"

    elasticsearch_snapshots_exporter:
        image: sunbird/prometheus-jsonpath-exporter:v0.0.1
        ports:
            - 9159:9158
        networks:
            - monitoring
        command: /etc/elasticsearch_snapshots_exporter/config.yml
        configs:
            - source: elasticsearch_snapshots_exporter_config.yml
              target: /etc/elasticsearch_snapshots_exporter/config.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ jsonpath_exporter.reservation_memory }}"
                limits:
                    memory: "{{ jsonpath_exporter.limit_memory }}"

    data_backup_azure_blob_exporter:
        image: sunbird/prometheus-azure-blob-exporter:v0.0.2
        ports:
            - 9358:9358
        networks:
            - monitoring
        command: /etc/data_backup_azure_blob_exporter/config.yml
        configs:
            - source: data_backup_azure_blob_exporter_config.yml
              target: /etc/data_backup_azure_blob_exporter/config.yml
        deploy:
            resources:
                reservations:
                    memory: "{{ azure_blob_exporter.reservation_memory }}"
                limits:
                    memory: "{{ azure_blob_exporter.limit_memory }}"

    grafana:
        image: grafana/grafana:4.5.0
        ports:
            - "3001:3000"
        networks:
            - monitoring
        volumes:
            - /var/dockerdata/grafana:/var/lib/grafana
        command: -e "PROMETHEUS_ENDPOINT=http://prometheus:9090/prometheus"
        environment:
            - GF_SERVER_ROOT_URL=http://grafana.local.com/grafana
            - GF_SECURITY_ADMIN_PASSWORD={{grafana_admin_password}}
        labels:
            com.docker.stack.namespace: "monitoring"
            com.docker.service.name: "grafana"
        deploy:
            replicas: 1
            placement:
                constraints:
                    - "node.labels.grafana==1"

    cassandra_jmx_exporter:
        image: sunbird/cassandra_jmx_exporter:0.11
        ports:
            - "5556:5556"
        environment:
            - JAVA_OPTS=-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=5555
                -Djava.util.logging.config.file=/opt/app/logging.properties
        configs:
            - source: jmx_httpserver.yml
              target: /opt/app/jmx_httpserver.yml
        networks:
            - monitoring

    logstash:
        image: sunbird/logstash:2.4.1
        command: logstash -f /conf/logstash.conf
        environment:
            - LOGSPOUT=ignore
            - LS_HEAP_SIZE={{ monitor_logstash.heap_size }}
        configs:
            - source: monitor_logstash.conf
              target: /conf/logstash.conf
            - source: monitor_logstash_grok_patterns
              target: /conf/grok_patterns
        ports:
            - '51416:51415'
            - '5045:5044'
        networks:
            - monitoring
        deploy:
            replicas: {{ monitor_logstash.replicas }}
            resources:
                reservations:
                    memory: "{{ monitor_logstash.reservation_memory }}"
                limits:
                    memory: "{{ monitor_logstash.limit_memory }}"

    logspout:
        image: gliderlabs/logspout:v3.2.3
        command: syslog+tcp://monitor_logstash:51415?filter.name=*_proxy.*
        deploy:
            mode: global
        environment:
            - SYSLOG_FORMAT=rfc3164
        depends_on:
            - monitor_logstash
        volumes:
            - '/var/run/docker.sock:/tmp/docker.sock'
        networks:
            - monitoring

configs:
  prometheus.yml:
    external: true
  alertrules.nodes:
    external: true
  alertrules.task:
    external: true
  alertrules.es:
    external: true
  alertrules.logs:
    external: true
  alertrules.backups:
    external: true
  alertrules.postgresql:
    external: true
  alertrules.kong:
    external: true
  alertrules.services:
    external: true
  alertrules.docker:
    external: true
  alertmanagerconfig.yml:
    external: true
  blackboxconfig.yml:
    external: true
  postgresmasterqueries.yml:
    external: true
  postgresslavequeries.yml:
    external: true
  statsd_mapping.yml:
    external: true
  jmx_httpserver.yml:
    external: true
  kong_cluster_exporter_config.yml:
    external: true
  elasticsearch_snapshots_exporter_config.yml:
    external: true
  data_backup_azure_blob_exporter_config.yml:
    external: true
  monitor_logstash.conf:
    external: true
  monitor_logstash_grok_patterns:
    external: true

networks:
    monitoring:
        external: true
    api-manager_default:
        external: true
    application_default:
        external: true
    logger:
        external: true
