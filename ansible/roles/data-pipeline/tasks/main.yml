---
# tasks file for data-pipeline
- name: Git checkout
  git:
    repo: 'https://github.com/Sunbird-Ed/ml-analytics-service.git'
    dest: "/opt/sparkjobs/ml-analytics-service"
    version: main

- name: CREATE THE USER
  user:
    name: "{{ USER }}"
    state: present
    home: "/home/{{ USER }}"
    shell: /bin/bash

- name: Ensure the locale exists
  locale_gen:
    name: en_US.UTF-8
    state: present

- name: set as default locale
  command: locale-gen en_US.UTF-8

- name: ADD THE REPOSITORY
  shell: sudo apt-add-repository universe
 
- name: INSTALL THE PACKGES
  apt:  
    update_cache: yes
    name: "{{ item }}"
    state: present
  loop:
    - openjdk-8-jdk
    - software-properties-common
    - python3-pip
    - python3.6-dev
    - python3.6-venv
    - python-virtualenv
    - zip
    - unzip

- name: CHANGE THE OWNERSHIP FOR THIS {{ BASEPATH }} DIRECTORY
  file:
    path: "{{ item }}"
    mode: "0755"
    recurse: yes
    state: directory
    owner: "{{ USER }}"
    group: "{{ USER }}"
  loop:
    - "{{ BASEPATH }}"
    - "{{ WORKDIR }}"
    - "{{ WORKDIR }}/faust_as_service"

- name: Change user and create working directory under opt dir and install python virtual environment
  shell: "cd {{ WORKDIR }} && virtualenv --python=python3.6 spark_venv"
  become: yes
  become_user: "{{ USER }}"

- name: Create necessary logs folders for pipeline
  become_user: "{{ USER }}"
  become: yes
  shell: "cd {{ WORKDIR }} && mkdir -p logs/observation/status logs/observation_evidence logs/project logs/survey logs/survey_evidence"

- name: Initiate virtualenv
  pip:
    virtualenv: "{{WORKDIR }}/spark_venv"
    virtualenv_python: python3.6
    requirements: "{{ WORKDIR }}/ml-analytics-service/requirements.txt"

- name: Creating a faust service shell file in executable mode
  copy:
    dest: "{{ WORKDIR }}/faust_as_service/faust.sh"
    owner: "{{ USER }}"
    group: "{{ USER }}"	
    mode: 0777
    content: |
      #!/bin/sh
      export LANG=C.UTF-8
      export LC_ALL=C.UTF-8
      /opt/sparkjobs/spark_venv/bin/python /opt/sparkjobs/source/\$1.py --workdir /opt/sparkjobs/source/\$2 worker -l info

- name: CREATION THE SYSTEMD faust_observation FILE
  copy:
    src: files/{{ item }}
    dest: "{{ SYSTEMDDIR }}"
  with_items:
    - faust_observation.service
    - faust_observation_evidence.service
    - faust_survey.service
    - faust_survey_evidence.service

- name: SYSTEMD DAEMON-RELOAD
  systemd:
    daemon_reload: yes

- name: START AND ENABLE THE SERVICE
  systemd:
    name: "{{ item }}"
    enabled: yes
    state: started
  loop:
    - faust_observation.service 
    - faust_observation_evidence.service 
    - faust_survey.service 
    - faust_survey_evidence.service

- name: CREATE THE CRON
  shell: |
    crontab -l 2>/dev/null
    echo '30 18 * * * . /opt/sparkjobs/source/spark_batch_jobs_run.sh >> /tmp/batch.log' | crontab -
  become_user: "{{ USER }}"
  become: yes
