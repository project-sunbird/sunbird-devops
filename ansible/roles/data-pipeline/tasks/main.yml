---
# tasks file for data-pipeline
- name: Git checkout
  git:
    repo: 'https://github.com/Sunbird-Ed/ml-analytics-service.git'
    dest: "/opt/sparkjobs/ml-analytics-service"
    version: main

- name: CREATE THE USER
  user:
    name: "{{ USER }}"
    state: present
    home: "/home/{{ USER }}"
    shell: /bin/bash

- name: Ensure the locale exists
  locale_gen:
    name: en_US.UTF-8
    state: present

- name: set as default locale
  command: locale-gen en_US.UTF-8

- name: ADD THE REPOSITORY
  shell: sudo apt-add-repository universe
 
- name: INSTALL THE PACKGES
  apt:  
    update_cache: yes
    name: "{{ item }}"
    state: present
  loop:
    - openjdk-8-jdk
    - software-properties-common
    - python3-pip
    - python3.6-dev
    - python3.6-venv
    - python-virtualenv
    - zip
    - unzip

- name: CHANGE THE OWNERSHIP FOR THIS {{ BASEPATH }} DIRECTORY
  file:
    path: "{{ item }}"
    mode: "0755"
    recurse: yes
    state: directory
    owner: "{{ USER }}"
    group: "{{ USER }}"
  loop:
    - "{{ BASEPATH }}"
    - "{{ WORKDIR }}"
    - "{{ WORKDIR }}/faust_as_service"

- name: Change user and create working directory under opt dir and install python virtual environment
  shell: "cd {{ WORKDIR }} && virtualenv --python=python3.6 spark_venv"
  become: yes
  become_user: "{{ USER }}"

- name: Create necessary logs folders for pipeline
  become_user: "{{ USER }}"
  become: yes
  shell: "cd {{ WORKDIR }} && mkdir -p logs/observation/status logs/observation_evidence logs/project logs/survey logs/survey_evidence"

- name: Initiate virtualenv
  pip:
    virtualenv: "{{WORKDIR }}/spark_venv"
    virtualenv_python: python3.6
    requirements: "{{ WORKDIR }}/ml-analytics-service/requirements.txt"

- name: Creating a faust service shell file in executable mode
  copy:
    dest: "{{ WORKDIR }}/faust_as_service/faust.sh"
    owner: "{{ USER }}"
    group: "{{ USER }}"	
    mode: 0777
    content: |
      #!/bin/sh
      export LANG=C.UTF-8
      export LC_ALL=C.UTF-8
      /opt/sparkjobs/spark_venv/bin/python /opt/sparkjobs/source/\$1.py --workdir /opt/sparkjobs/source/\$2 worker -l info

- name: CREATION THE SYSTEMD faust_observation FILE
  copy:
    dest: "{{ SYSTEMDDIR }}/faust_observation.service"
    content: |
      [Unit]
      Description=Faust Observations Service
      [Service]
      User=data-pipeline
      Group=data-pipeline
      Restart=on-failure
      Type=simple
      ExecStart=/opt/sparkjobs/faust_as_service/faust.sh  observations/observation_streaming observations/
      [Install]
      WantedBy=multi-user.target

- name: CREATION THE SYSTEMD faust_observation_evidence FILE
  copy:
    dest: "{{ SYSTEMDDIR }}/faust_observation_evidence.service"
    content: |
      [Unit]
      Description=Faust Observations Evidence Service
      [Service]
      User=data-pipeline
      Group=data-pipeline
      Restart=on-failure
      Type=simple
      ExecStart=/opt/sparkjobs/faust_as_service/faust.sh  observations/observation_evidence_streaming observations/
      [Install]
      WantedBy=multi-user.target

- name: CREATION THE SYSTEMD faust_survey FILE
  copy:
    dest: "{{ SYSTEMDDIR }}/faust_survey.service"
    content: |
      [Unit]
      Description=Faust Survey Service
      [Service]
      User=data-pipeline
      Group=data-pipeline
      Restart=on-failure
      Type=simple
      ExecStart=/opt/sparkjobs/faust_as_service/faust.sh  survey/sl_py_survey_streaming survey/
      [Install]
      WantedBy=multi-user.target

- name: CREATION THE SYSTEMD faust_survey_evidence FILE
  copy:
    dest: "{{ SYSTEMDDIR }}/faust_survey_evidence.service"
    content: |
      [Unit]
      Description=Faust Survey Evidence Service
      [Service]
      User=data-pipeline
      Group=data-pipeline
      Restart=on-failure
      Type=simple
      ExecStart=/opt/sparkjobs/faust_as_service/faust.sh  survey/evidence/sl_py_survey_evidence_streaming survey/evidence/
      [Install]
      WantedBy=multi-user.target

- name: SYSTEMD DAEMON-RELOAD
  systemd:
    daemon_reload: yes

- name: START AND ENABLE THE SERVICE
  systemd:
    name: "{{ item }}"
    enabled: yes
    state: started
  loop:
    - faust_observation.service 
    - faust_observation_evidence.service 
    - faust_survey.service 
    - faust_survey_evidence.service

- name: CREATE THE CRON
  shell: |
    crontab -l 2>/dev/null
    echo '30 18 * * * . /opt/sparkjobs/source/spark_batch_jobs_run.sh >> /tmp/batch.log' | crontab -
  become_user: "{{ USER }}"
  become: yes
